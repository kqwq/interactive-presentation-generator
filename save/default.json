{
  "$schema": "../src/save-schema.json",
  "numSlides": 3,
  "textBoxes": [
    [1, "Masked Language Models", 960, 540, "95px Arial"],
    [1, "Welcome to the AI\nCompetition Club!", 140, 135, "25px Fira Sans", "#333", "left"],
    [
      1,
      "Each slide has a minigame\nScan the QR code to join",
      1560,
      920,
      "25px Fira Sans",
      "#333",
      "left"
    ],
    [1, "Kyle Wells\nDate: <t:1743021000>", 200, 745, "bold 34px monospace", "#bbb", "left"],
    [2, "Outline", 80, 170, "80px Arial", "black", "left"],
    [
      2,
      "• Definitions\n\n• BERT\n\n• Code project #1\n\n• Code project #2\n\n• Test your knowledge!",
      160,
      310,
      "50px Arial",
      "black",
      "left"
    ],
    [3, "Definitions", 80, 170, "80px Arial", "black", "left"],
    [
      3,
      "• Masked Language Model - predicts a “masked” word in a sentence\ne.g. “I picked up a [MASK] from the table.”\n• Attention - determines importance of each word relative to each other\n• Transformer - machine learning architecture that uses a “multi-head\nattention mechanism”\n• BERT - a language model developed by Google in 2018\n• Applications of MLMs",
      160,
      310,
      "50px Arial",
      "black",
      "left",
      1.5
    ],
    [4, "Predicting a masked word", 80, 170, "80px Arial", "black", "left"],
    [6, "Predicting a masked word", 80, 170, "80px Arial", "black", "left"],
    [8, "Predicting a masked word", 80, 170, "80px Arial", "black", "left"],
    [10, "Predicting a masked word", 80, 170, "80px Arial", "black", "left"],
    [12, "Code Project #1 - Masked Language Model", 80, 170, "80px Arial", "black", "left"],
    [13, "Code Project #2 - Text Generator", 80, 170, "80px Arial", "black", "left"],
    [14, "Quiz", 960, 540, "80px Arial", "black", "center"],
    [32, "Gracias por participar!", 20, 1040, "15px Fira Sans", "#bbb", "left"],
    [32, "That's all folks", 960, 540, "80px Arial", "black", "center"]
  ],
  "platforms": [
    [1, 120, 202.5, 120, 67.5],
    [1, 600, 270, 240, 67.5],
    [1, 960, 202.5, 600, 67.5],
    [1, 0, 405, 120, 135],
    [1, 240, 590.625, 120, 67.5],
    [1, 0, 742.5, 120, 135],
    [1, 480, 877.5, 120, 135],
    [1, 840, 742.5, 120, 202.5],
    [1, 1080, 675, 120, 135],
    [1, 1320, 810, 120, 67.5],
    [1, 1680, 607.5, 120, 135],
    [1, -195, 0, 200, 1080],
    [1, 1915, 0, 200, 1080],
    [1, 0, 1075, 1920, 200],
    [2, 1040, 40, 800, 20],
    [2, 1040, 820, 800, 20],
    [2, 1040, 40, 20, 800],
    [2, 1820, 40, 20, 800],
    [12, 0, 1070, 1920, 50, "gray"],
    [13, 0, 1070, 1920, 50, "gray"],
    [14, 0, 1070, 1920, 50, "gray"],
    [32, 480, 1070, 960, 50, "gray"]
  ],
  "singleResponseSlides": [
    {
      "slide": 4,
      "prompt": "The capital of France is [MASK]."
    },
    {
      "slide": 6,
      "prompt": "I picked up a [MASK] from the table."
    },
    {
      "slide": 8,
      "prompt": "Upon completion of all required courses, a college student receives a [MASK]."
    },
    {
      "slide": 10,
      "prompt": "The largest land animal is the [MASK]."
    }
  ],
  "kahootQuestions": [
    {
      "slide": 15,
      "question": "Which company developed the BERT language model?",
      "answers": ["Google", "Facebook", "Microsoft", "Amazon"],
      "correctAnswer": "Google"
    },
    {
      "slide": 17,
      "question": "Which Python class turns a string into a list of tokens?",
      "answers": ["AutoTokenizer", "TFBertForMaskedLM", "from_pretrained", "transformers"],
      "correctAnswer": "AutoTokenizer"
    },
    {
      "slide": 19,
      "question": "Which of the following processes allows language models to run in only\na few seconds?",
      "answers": [
        "Inference",
        "Training",
        "The Software Development Lifecycle",
        "Test-Driven Development"
      ],
      "correctAnswer": "Inference"
    },
    {
      "slide": 21,
      "question": "What does \"softmax\" do in tensorflow?",
      "answers": [
        "Normalizes log probabilities",
        "Converts a tensor to a numpy array",
        "Finds the \"softest\" value in a tensor",
        "Computes new log probabilities"
      ],
      "correctAnswer": "Normalizes log probabilities"
    },
    {
      "slide": 23,
      "question": "GPT-2 (including ChatGPT) and BERT are based on the same fundamental\nmachine learning architecture also known as?",
      "answers": ["Transformer", "Neural Network", "x86", "Word2vec"],
      "correctAnswer": "Transformer"
    },
    {
      "slide": 25,
      "question": "If x is not already defined, is this valid Python 3.8 code?\nx := result.logits[0, maskIndex]",
      "answers": ["Yes", "No"],
      "correctAnswer": "No"
    },
    {
      "slide": 27,
      "question": "When was BERT published?",
      "answers": ["1916", "2015", "2016", "2018"],
      "correctAnswer": "2018"
    },
    {
      "slide": 29,
      "question": "What is objectively the best programming language?",
      "answers": ["C++", "Python", "SNOBOL", "Rust"],
      "correctAnswer": "<all>"
    }
  ],
  "kahootFinalStandingsSlide": 31
}
